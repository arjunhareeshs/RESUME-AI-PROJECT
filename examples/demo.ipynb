{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Processing PDF: C:\\RESUME-AI-PROJECT\\data\\uploads\\resume1.pdf\n",
            "üîÑ Extracting text from PDF...\n",
            "üîÑ Cleaning text...\n",
            "üîÑ Segmenting resume...\n",
            "‚úÖ Successfully processed and saved to: C:\\RESUME-AI-PROJECT\\data\\processed_texts\\resume1.jsonl\n",
            "üìä Segmented sections found: ['PROFILE', 'TECH SKILLS', 'SOFT SKILLS', 'EDUCATION', 'OTHER']\n",
            "\n",
            "üìã Preview of segmented text:\n",
            "\n",
            "PROFILE:\n",
            "  II m a student passionate about AI and BI  exploring how artificial CONTACT intelligence can transfo...\n",
            "\n",
            "TECH SKILLS:\n",
            "  Hand sign preedictor c programming It analysis the sign and report what it is. c ++ (basics) it is b...\n",
            "\n",
            "SOFT SKILLS:\n",
            "  Certified by Avinash Academy on C C++ python basics Teamwork NPTEL :Buisness fundamentals for entrep...\n",
            "\n",
            "EDUCATION:\n",
            "  Leadership school : M.S . Vidyalaya Matriculation school 2011 - 2024 Critical Thinking percentage : ...\n",
            "\n",
            "OTHER:\n",
            "  ARJUN HAREESH S AI AND BI DEVELOPER\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path for imports (go up one level from examples/)\n",
        "project_root = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "from eval.evaluator import process_resume, extract_text_from_pdf, clean_resume_text, segment_resume\n",
        "\n",
        "# --------------------------\n",
        "# CONFIG\n",
        "# --------------------------\n",
        "# Input PDF file\n",
        "input_pdf = Path(r\"C:\\RESUME-AI-PROJECT\\data\\uploads\\resume1.pdf\")\n",
        "\n",
        "# Output directory for JSONL files\n",
        "output_dir = Path(r\"C:\\RESUME-AI-PROJECT\\data\\processed_texts\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --------------------------\n",
        "# MAIN PROCESSING\n",
        "# --------------------------\n",
        "if not input_pdf.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå Input file not found: {input_pdf}\")\n",
        "\n",
        "print(f\"üìÑ Processing PDF: {input_pdf}\")\n",
        "\n",
        "try:\n",
        "    # Extract text from PDF\n",
        "    print(\"üîÑ Extracting text from PDF...\")\n",
        "    raw_text = extract_text_from_pdf(str(input_pdf))\n",
        "    \n",
        "    # Clean the text\n",
        "    print(\"üîÑ Cleaning text...\")\n",
        "    cleaned_text = clean_resume_text(raw_text)\n",
        "    \n",
        "    # Segment the resume\n",
        "    print(\"üîÑ Segmenting resume...\")\n",
        "    segmented_data = segment_resume(cleaned_text)\n",
        "    \n",
        "    # Create output data structure\n",
        "    output_data = {\n",
        "        \"source_file\": str(input_pdf),\n",
        "        \"raw_text\": raw_text,\n",
        "        \"cleaned_text\": cleaned_text,\n",
        "        \"segmented_text\": segmented_data,\n",
        "        \"extraction_method\": \"pdfplumber\"\n",
        "    }\n",
        "    \n",
        "    # Save as JSONL format\n",
        "    output_file = output_dir / f\"{input_pdf.stem}.jsonl\"\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(json.dumps(output_data, ensure_ascii=False) + '\\n')\n",
        "    \n",
        "    print(f\"‚úÖ Successfully processed and saved to: {output_file}\")\n",
        "    print(f\"üìä Segmented sections found: {list(segmented_data.keys())}\")\n",
        "    \n",
        "    # Show preview of segmented data\n",
        "    print(\"\\nüìã Preview of segmented text:\")\n",
        "    for section, content in segmented_data.items():\n",
        "        if content.strip():\n",
        "            print(f\"\\n{section}:\")\n",
        "            print(f\"  {content[:100]}{'...' if len(content) > 100 else ''}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error processing file: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (venv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
