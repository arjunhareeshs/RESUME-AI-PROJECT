{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb41ade7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Extracting text from: data\\uploads\\resume1.pdf\n",
      "‚úÖ Successfully extracted 1727 characters\n",
      "‚úÖ Cleaned text: 1726 characters\n",
      "‚úÖ Segmented into 5 sections\n",
      "\n",
      "üìã Raw text preview (first 500 chars):\n",
      "ARJUN HAREESH S\n",
      "AI AND BI DEVELOPER\n",
      "PROFILE\n",
      "II‚Äôm a student passionate about AI and BI, exploring how artificial\n",
      "CONTACT\n",
      "intelligence can transform industries and how business\n",
      "intelligence tools drive data-driven decisions. I'm excited to learn\n",
      "+91 6383876196\n",
      "and apply new technologies to solve real-world problems and\n",
      "contribute to innovations in these fields.\n",
      "arjunhareeshsenthil@gmail.com\n",
      "tamilnadu,India PROJECTS\n",
      "www.linkedin.com/in/arjun\n",
      "Real time resume score analyser\n",
      "hareesh25\n",
      "https://github.\n",
      "\n",
      "üìã Segmented sections:\n",
      "\n",
      "PROFILE:\n",
      "  II m a student passionate about AI and BI  exploring how artificial CONTACT intelligence can transfo...\n",
      "\n",
      "TECH SKILLS:\n",
      "  Hand sign preedictor c programming It analysis the sign and report what it is. c ++ (basics) it is b...\n",
      "\n",
      "SOFT SKILLS:\n",
      "  Certified by Avinash Academy on C C++ python basics Teamwork NPTEL :Buisness fundamentals for entrep...\n",
      "\n",
      "EDUCATION:\n",
      "  Leadership school : M.S . Vidyalaya Matriculation school 2011 - 2024 Critical Thinking percentage : ...\n",
      "\n",
      "OTHER:\n",
      "  ARJUN HAREESH S AI AND BI DEVELOPER\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for PDF extraction\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from eval.evaluator import extract_text_from_pdf, clean_resume_text, segment_resume\n",
    "import json\n",
    "\n",
    "# Extract text from PDF\n",
    "pdf_path = r\"data\\uploads\\resume1.pdf\"\n",
    "print(f\"üìÑ Extracting text from: {pdf_path}\")\n",
    "\n",
    "try:\n",
    "    # Extract text using our robust PDF extraction function\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    print(f\"‚úÖ Successfully extracted {len(raw_text)} characters\")\n",
    "    \n",
    "    # Clean the text\n",
    "    cleaned_text = clean_resume_text(raw_text)\n",
    "    print(f\"‚úÖ Cleaned text: {len(cleaned_text)} characters\")\n",
    "    \n",
    "    # Segment the resume\n",
    "    segmented_data = segment_resume(cleaned_text)\n",
    "    print(f\"‚úÖ Segmented into {len(segmented_data)} sections\")\n",
    "    \n",
    "    # Display the extracted text preview\n",
    "    print(f\"\\nüìã Raw text preview (first 500 chars):\")\n",
    "    print(raw_text[:500])\n",
    "    \n",
    "    print(f\"\\nüìã Segmented sections:\")\n",
    "    for section, content in segmented_data.items():\n",
    "        if content.strip():\n",
    "            print(f\"\\n{section}:\")\n",
    "            print(f\"  {content[:100]}{'...' if len(content) > 100 else ''}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98ac358e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ PDF: data/uploads/resume1.pdf\n",
      "üìä Pages: 1\n",
      "\n",
      "üìù Text:\n",
      "SOFT SKILLS\n",
      "CONTACT\n",
      "TECH SKILLS\n",
      "LANGUAGES\n",
      "+91 6383876196\n",
      "arjunhareeshsenthil@gmail.com\n",
      "tamilnadu,India\n",
      "www.linkedin.com/in/arjun\n",
      "hareesh25\n",
      "c programming\n",
      "c ++ (basics)\n",
      "python\n",
      "java (basics)\n",
      "Data strucutres\n",
      "microsoft (excel , powerpoint)\n",
      "data science\n",
      "machine learning\n",
      "deeplearning\n",
      "web development \n",
      "ARJUN HAREESH S\n",
      "AI AND BI DEVELOPER\n",
      "PROJECTS\n",
      "EDUCATION\n",
      "PROFILE\n",
      "II‚Äôm a student passionate about AI and BI, exploring how artificial\n",
      "intelligence \n",
      "can \n",
      "transform \n",
      "industries \n",
      "and \n",
      "how \n",
      "business\n",
      "intelligence tools drive data-driven decisions. I'm excited to learn\n",
      "and apply new technologies to solve real-world problems and\n",
      "contribute to innovations in these fields.\n",
      "2011 - 2024\n",
      "school : M.S . Vidyalaya Matriculation school\n",
      "2024 - 2026\n",
      "Bannri  amman institute of technology\n",
      "degree : B.TECH - Artificial  intelligence and data science\n",
      "cGPA: 8.28\n",
      "percentage :\n",
      "It analysis the real time score of a resume uploaded\n",
      "it  is  built  using  python  language \n",
      "webframeworks  are  streamlit and SQLlite\n",
      "93 %   (10 th std)  \n",
      "percentage : 90 %   (12 th std)  \n",
      "Real  time resume score analyser\n",
      "Teamwork \n",
      "Time Management\n",
      "Leadership \n",
      "Critical Thinking\n",
      "Hand sign preedictor \n",
      "It analysis the sign and report what it is.\n",
      "it  is  built  using  python  language  and YOLOv8 model\n",
      "webframeworks  are gradio and SQLlite\n",
      "Text  generator \n",
      "It generates text using a transformer model\n",
      "it  is  built  using  python  language ,tennsorflow and pytorch\n",
      "webframeworks  are gradio and SQLlite\n",
      "Image classifier\n",
      "It classifies image of butterfly using CNN\n",
      "it  is  built  using  python  language ,tennsorflow and pytorch\n",
      "webframeworks  are gradio and SQLlite\n",
      "CERTIFICATIONS\n",
      "Certified by Avinash Academy on C,C++,python basics\n",
      "NPTEL :Buisness fundamentals for entrepeuners\n",
      "https://github.com/arjunha\n",
      "reeshs\n",
      "English\n",
      "Tamil\n",
      "\n",
      "[{'kind': 2, 'xref': 12, 'from': Rect(38.01626968383789, 348.3509521484375, 106.46958923339844, 361.08642578125), 'uri': 'https://github.com/KishoreR2k7', 'id': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'kind': 2,\n",
       "  'xref': 12,\n",
       "  'from': Rect(38.01626968383789, 348.3509521484375, 106.46958923339844, 361.08642578125),\n",
       "  'uri': 'https://github.com/KishoreR2k7',\n",
       "  'id': ''}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clear and simple PDF loading\n",
    "import fitz\n",
    "\n",
    "# Load PDF\n",
    "pdf_path = \"data/uploads/resume1.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "print(f\"üìÑ PDF: {pdf_path}\")\n",
    "print(f\"üìä Pages: {doc.page_count}\")\n",
    "\n",
    "# Get first page text\n",
    "page = doc[0]\n",
    "text = page.get_text()\n",
    "\n",
    "print(f\"\\nüìù Text:\")\n",
    "print(text)\n",
    "\n",
    "pagee = page.get_links()\n",
    "\n",
    "print(pagee)\n",
    "# Close\n",
    "doc.close()\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "display(pagee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fef7d39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C a r e e r  O b j e c t i v e\n",
      "E d u c a t i o n\n",
      "S k i l l s  S u m m a r y\n",
      "L a n g u a g e\n",
      "S k i l l  a n d  e x p e r t i s e\n",
      "100%\n",
      "75%\n",
      "I ‚Äô m  a n  u n d e r g r a d u a t e  C o m p u t e r  S c i e n c e  a n d\n",
      "E n g i n e e r i n g  s t u d e n t  a t  B a n n a r i  A m m a n\n",
      "I n s t i t u t e  o f  T e c h n o l o g y ,  S a t h y .  I ‚Äô m  c u r r e n t l y  p a r t\n",
      "o f  t h e  A I  W o r k f o r c e  t e a m ,  w h e r e  I  w o r k  o n\n",
      "p r o j e c t s  i n v o l v i n g  M a c h i n e  L e a r n i n g  a n d  D e e p\n",
      "L e a r n i n g .  I ‚Äô m  p a s s i o n a t e  a b o u t  b u i l d i n g\n",
      "i n t e l l i g e n t  s y s t e m s  a n d  e x p l o r i n g  h o w  A I  c a n\n",
      "s o l v e  r e a l - w o r l d  p r o b l e m s .\n",
      "A b o u t  M e\n",
      "s r i v a r s h a p . c s 2 4 @ b i t s a t h y . a c . i n\n",
      "s r i v a r s h a g k g k @ g m a i l . c o m\n",
      "7 5 4 8 8  4 2 3 1 0\n",
      "B o a r d  r e s u l t s :\n",
      "           1 0  ( H S C )  -  9 2 %  ( 2 0 2 2 )\n",
      "t h (\n",
      "           1 2  ( S S L C )  - 9 4 . 5 %  ( 2 0 2 4 )\n",
      "t h\n",
      "1 6 4 , s a r a t h a m b a l  n a g a r ,\n",
      "K a r u m a t h a m p a t t i .\n",
      "T o  o b t a i n  a  c h a l l e n g i n g  p o s i t i o n  i n  t h e  f i e l d  o f  A r t i f i c i a l\n",
      "I n t e l l i g e n c e  a n d  C o m p u t e r  S c i e n c e  w h e r e  I  c a n  a p p l y  m y\n",
      "k n o w l e d g e  o f  M a c h i n e  L e a r n i n g  a n d  D e e p  L e a r n i n g  t o\n",
      "d e v e l o p  i n n o v a t i v e  a n d  e f f i c i e n t  s o l u t i o n s .  I  a i m  t o\n",
      "c o n t r i b u t e  t o  i m p a c t f u l  p r o j e c t s  t h a t  e n h a n c e  t e c h n o l o g y -\n",
      "d r i v e n  p r o b l e m  s o l v i n g  w h i l e  c o n t i n u o u s l y  l e a r n i n g  a n d\n",
      "e x p a n d i n g  m y  t e c h n i c a l  a n d  p r o f e s s i o n a l  s k i l l s .\n",
      "E n g l i s h   ( r , w , s )\n",
      "T a m i l   ( r , w , s )\n",
      "T e l u g u   ( s )\n",
      "C  ,  P y t h o n ,  U I / U X ,  J a v a ( F o u n d a t i o n a l )\n",
      "C r e a t i v i t y\n",
      "M a c h i n e  L e a r n i n g\n",
      "P r o b l e m  S o l v i n g\n",
      "C r i t i c a l  T h i n k i n g\n",
      "L e a d e r s h i p\n",
      "F a c e  R e c o g n i t i o n  S y s t e m  ‚Äì  I m p l e m e n t e d  a  f a c i a l\n",
      "r e c o g n i t i o n  m o d e l  u s i n g  d e e p  l e a r n i n g  f o r  r e a l - t i m e\n",
      "i d e n t i f i c a t i o n  a n d  v e r i f i c a t i o n .\n",
      "T e x t  G e n e r a t i o n  u s i n g  M i s t r a l  A I  ‚Äì  B u i l t  a  t r a n s f o r m e r -\n",
      "b a s e d  t e x t  g e n e r a t i o n  m o d e l  l e v e r a g i n g  t h e  M i s t r a l - 7 B\n",
      "a r c h i t e c t u r e  f o r  n a t u r a l  l a n g u a g e  g e n e r a t i o n .\n",
      "K o n g u  V e l l a l a r  M a t r i . H r . S e c . S c h o o l , k a r u m a t h a m p a t t i\n",
      "B . E  - C S E ( e x p e c t e d  t o  g r a d u a t e  a t  2 0 2 8 )\n",
      "C G P A :  8 . 0 9  ( u p t o  2  s e m e s t e r )\n",
      "n d\n",
      "B a n n a r i  A m m a n  I n s t i t u t e  O f  T e c h n o l o g y , S a t h y a m a n g a l a m\n",
      "M a c h i n e  l e a r n i n g\n",
      "D e e p  l e a r n i n g\n",
      "P r o d u c t  D e s i g n e r\n",
      "                                                                      A S P I R I N G    G E N E R A T I V E  A I   E N G I N E E R\n",
      "S R I   V A R S H A  P\n",
      "C o n t a c t s\n",
      "P r o j e c t s\n",
      "O b j e c t  D e t e c t i o n  u s i n g  Y O L O v 8  ‚Äì  D e v e l o p e d  a n  o b j e c t\n",
      "d e t e c t i o n  p i p e l i n e  u s i n g  Y O L O v 8  t o  i d e n t i f y  a n d  c l a s s i f y\n",
      "m u l t i p l e  o b j e c t s  i n  i m a g e s  a n d  v i d e o s .\n",
      "C l a s s i f i c a t i o n  M o d e l  u s i n g  D a t a s e t  ‚Äì  T r a i n e d  a n d  e v a l u a t e d\n",
      "m a c h i n e  l e a r n i n g  m o d e l s  f o r  p r e d i c t i v e  c l a s s i f i c a t i o n  t a s k s  u s i n g\n",
      "r e a l - w o r l d  d a t a s e t s .\n",
      "R e f e r e n c e :\n",
      "G i t h u b :  h t t p s : / / g i t h u b . c o m / s r i v a r s h a 1 4 0 9\n",
      " \n",
      "D E C L A R A T I O N :   I  h e r e b y  d e c l a r e  t h a t  a l l  i n f o r m a t i o n\n",
      "g i v e n  a b o v e  a r e  t r u e  t o  t h e  b e s t  o f  m y  k n o w l e d g e .\n",
      "                                                                                S r i  V a r s h a  P\n",
      "A r e a  o f  I n t e r e s t\n",
      "C o m p u t e r  v i s i o n\n",
      "G e n e r a t i v e  A I\n",
      "N e u r a l  N e t w o r k  A r c h i t e c t u r e s\n",
      "L i n k e d i n :  h t t p s : / / w w w . l i n k e d i n . c o m / i n / s r i - v a r s h a - p - 0 8 b 3 6 1 3 6 9 /\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "# Load the PDF file (using raw string to avoid Unicode escape error)\n",
    "reader = PdfReader(r\"data\\uploads\\resume2.pdf\")\n",
    "\n",
    "# Access a specific page (e.g., first page)\n",
    "page = reader.pages[0]\n",
    "\n",
    "# Extract text from the page\n",
    "text = page.extract_text()\n",
    "\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08996087",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Add project root to path\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m)))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mextractor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResumeParser \u001b[38;5;66;03m# We'll borrow the entity/typo logic\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_html_from_pdf\u001b[39m(pdf_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# html_parser.py\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- We still need the parser's entity extraction logic ---\n",
    "# (Make sure this script is in the root or 'eval' folder and paths are correct)\n",
    "import sys\n",
    "import os\n",
    "# Add project root to path\n",
    "sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))\n",
    "from extractor.parser import ResumeParser # We'll borrow the entity/typo logic\n",
    "\n",
    "\n",
    "def extract_html_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses PyMuPDF (fitz) to extract the content of a PDF as an HTML string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            html_content = \"\"\n",
    "            for page in doc:\n",
    "                # page.get_text(\"html\") preserves layout, fonts, and blocks\n",
    "                html_content += page.get_text(\"html\")\n",
    "        return html_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting HTML from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def parse_resume_html(html_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Uses BeautifulSoup to parse the HTML and extract structured data.\n",
    "    \"\"\"\n",
    "    if not html_content:\n",
    "        return {}\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "    structured_data = {}\n",
    "    \n",
    "    # --- 1. Define Section Headers ---\n",
    "    # We can use the same headers as our old parser\n",
    "    SECTION_HEADERS = [\n",
    "        \"PROFILE\", \"CONTACT\", \"TECH SKILLS\", \"SOFT SKILLS\", \n",
    "        \"LANGUAGES\", \"PROJECTS\", \"EDUCATION\", \"CERTIFICATIONS\"\n",
    "    ]\n",
    "    \n",
    "    # --- 2. Find all <b> (bold) tags ---\n",
    "    # In many resumes, headers are the only bolded text.\n",
    "    # This is a much more reliable signal than just the text itself.\n",
    "    \n",
    "    # We'll use a regex to find tags that contain our headers\n",
    "    header_regex = re.compile(r'|'.join(SECTION_HEADERS), re.IGNORECASE)\n",
    "    \n",
    "    # Find all tags (like <b>, <p>, <strong>) that contain header text\n",
    "    header_tags = soup.find_all(\n",
    "        lambda tag: tag.name in ['p', 'b', 'strong'] and \n",
    "                    header_regex.search(tag.get_text())\n",
    "    )\n",
    "\n",
    "    if not header_tags:\n",
    "        # Fallback if no bold headers are found\n",
    "        structured_data['raw_text'] = soup.get_text()\n",
    "        return structured_data\n",
    "\n",
    "    # --- 3. Loop through headers and extract content ---\n",
    "    sections = {}\n",
    "    for i, tag in enumerate(header_tags):\n",
    "        header_name = header_regex.search(tag.get_text()).group(0).upper()\n",
    "        \n",
    "        # Collect all text between this header and the next one\n",
    "        content = []\n",
    "        # find_next_siblings() gets all tags at the same level\n",
    "        for sibling in tag.find_next_siblings():\n",
    "            if sibling in header_tags:\n",
    "                # We've hit the next header, so stop\n",
    "                break\n",
    "            \n",
    "            # Get text from this sibling and all its children\n",
    "            sibling_text = sibling.get_text(separator=' ', strip=True)\n",
    "            if sibling_text:\n",
    "                content.append(sibling_text)\n",
    "        \n",
    "        sections[header_name] = \"\\n\".join(content)\n",
    "\n",
    "    structured_data[\"_raw_sections\"] = sections\n",
    "    \n",
    "    # --- 4. Use our old parser's logic for final cleanup ---\n",
    "    # We can reuse the typo fixing and entity extraction\n",
    "    parser = ResumeParser()\n",
    "    \n",
    "    # Re-join all text for entity extraction\n",
    "    all_text = \" \".join(sections.values())\n",
    "    \n",
    "    # Clean typos\n",
    "    all_text = parser._clean_text(all_text) \n",
    "    \n",
    "    # Extract entities\n",
    "    structured_data['contact'] = parser._extract_entities(sections)['contact']\n",
    "    structured_data['education'] = parser._extract_entities(sections).get('education')\n",
    "    \n",
    "    # Add cleaned sections\n",
    "    for section_name, text in sections.items():\n",
    "        structured_data[section_name.lower()] = text.strip()\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "# --- Main execution block to run the process ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    PDF_FILE = \"data/uploads/resume1.pdf\"  # Your input PDF\n",
    "    JSON_OUTPUT = \"data/resume_html_parsed.json\" # The final output\n",
    "    \n",
    "    print(f\"Extracting HTML from {PDF_FILE}...\")\n",
    "    html = extract_html_from_pdf(PDF_FILE)\n",
    "    \n",
    "    if html:\n",
    "        print(\"Parsing HTML with BeautifulSoup...\")\n",
    "        parsed_data = parse_resume_html(html)\n",
    "        \n",
    "        print(\"Saving structured data to JSON...\")\n",
    "        with open(JSON_OUTPUT, 'w', encoding='utf-8') as f:\n",
    "            json.dump(parsed_data, f, indent=4)\n",
    "        \n",
    "        print(f\"‚úÖ Success! Parsed data saved to {JSON_OUTPUT}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to extract HTML from {PDF_FILE}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
